{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da6d2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda\\envs\\CLIP\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "# 设置环境变量，让模型下载到D盘\n",
    "os.environ['HF_HOME'] = 'D:/huggingface_cache/models'\n",
    "os.environ['HF_HUB_CACHE'] = 'D:/huggingface_cache/hub'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '1000'\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.clip import CLIPProcessor, CLIPModel\n",
    "from transformers.models.auto.modeling_auto import AutoModel\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b23abe7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states shape: torch.Size([1, 6, 512])\n"
     ]
    }
   ],
   "source": [
    "#embed text\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "\n",
    "# 测试文本编码\n",
    "texts = [\"photo of a cloud\"]\n",
    "text_inputs = clip_processor(text=texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "#得到text的token id\n",
    "text_outputs = clip.text_model(**text_inputs)\n",
    "\n",
    "#print(\"Text outputs keys:\", text_outputs.keys())\n",
    "hidden_states = text_outputs.last_hidden_state\n",
    "print(\"Hidden states shape:\", hidden_states.shape)\n",
    "#hidden_states是[batch_size, sequence_length, hidden_size]\n",
    "text_embeds = clip.text_projection(hidden_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "16d4b362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden states shape: torch.Size([1, 197, 768])\n",
      "Image embeddings shape: torch.Size([1, 197, 512])\n"
     ]
    }
   ],
   "source": [
    "#embed image\n",
    "img = Image.open('G:/Research/James/VideoColBERT/project/try_data/text.jpg')\n",
    "if img.mode != 'RGB':\n",
    "    img = img.convert('RGB')\n",
    "inputs = clip_processor(images=img, return_tensors=\"pt\")\n",
    "vision_outputs = clip.vision_model(**inputs)\n",
    "hidden_states = vision_outputs.last_hidden_state\n",
    "print(\"Hidden states shape:\", hidden_states.shape)\n",
    "#hidden_states是[batch_size, sequence_length, hidden_size]\n",
    "#此处输出的sequence_length是197，因为将224*224的图像分割成16*16的patch\n",
    "#一共196个patch，加上一个CLS,一共197个token.\n",
    "image_embeds = clip.visual_projection(hidden_states)\n",
    "print(\"Image embeddings shape:\", image_embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be3acfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 512])\n",
      "torch.Size([1, 197, 512])\n",
      "tensor([[-0.0392, -0.0093, -0.0829,  ...,  0.0332, -0.0126, -0.0107],\n",
      "        [-0.1171, -0.0922, -0.1244,  ..., -0.0263, -0.0800, -0.0715],\n",
      "        [-0.1700, -0.1062, -0.1224,  ..., -0.0329, -0.0837, -0.0782],\n",
      "        [-0.1425, -0.0578, -0.0675,  ..., -0.0140, -0.0734, -0.0758],\n",
      "        [-0.0562, -0.0100, -0.0562,  ...,  0.0002, -0.0374, -0.0414],\n",
      "        [-0.0824, -0.0680, -0.0885,  ..., -0.0279, -0.0542, -0.0841]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "torch.Size([6, 197])\n",
      "tensor([0.0545, 0.0536, 0.0655, 0.0769, 0.0705, 0.0211],\n",
      "       grad_fn=<MaxBackward0>)\n",
      "torch.Size([6])\n",
      "tensor(0.3421, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Maxsim\n",
    "text_embeds_1 = text_embeds.unsqueeze(2).squeeze(0)\n",
    "print(text_embeds_1.shape)\n",
    "\n",
    "image_embeds_1 = image_embeds\n",
    "print(image_embeds_1.shape)\n",
    "\n",
    "sim_matrix = torch.cosine_similarity(text_embeds_1, image_embeds_1,dim=-1)\n",
    "print(sim_matrix)\n",
    "print(sim_matrix.shape)\n",
    "max_sim = torch.max(sim_matrix,dim=-1).values\n",
    "print(max_sim)\n",
    "print(max_sim.shape)\n",
    "score = torch.sum(max_sim)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b775ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.定义一个compute_score函数\n",
    "#2.遍历每个query和每个doc\n",
    "#3.对每个query,return top_3 doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CLIP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
